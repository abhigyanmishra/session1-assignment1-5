Apache Hadoop is 100% open source, and pioneered a fundamentally new way of storing and processing data. Instead of relying on expensive,
proprietary hardware and different systems to store and process data, Hadoop enables distributed parallel processing of huge amounts
of data across inexpensive, industry-standard servers that both store and process the data, and can scale without limits. With Hadoop,
no data is too big. And in today’s hyperconnected world where more and more data is being created every day, Hadoop ’ s
 
breakthrough advantages mean that businesses and organizations can now find value in data that was recently considered useless.
 
But what exactly is Hadoop, and what makes it so special? In it’s basic form, it is a way of storing enormous data sets across
distributed clusters of servers and then running "distributed" analysis applications in each cluster. It's designed to be robust, 
in that the Big
 
Data applications will continue to run even when failures occur in individual servers or clusters. It's also designed to be efficient,
because it doesn't require the applications to shuttle huge volumes of data across the network. It has two main parts; a data processing
 
framework (MapReduce) and a distributed file system (HDFS) for data storage. These are the components that are at the heart of Hadoop
and really make things happen.
